{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "\n",
    "\n",
    "\n",
    "閱讀以下兩篇文獻，了解決策樹原理，並試著回答後續的問題\n",
    "- [決策樹 (Decision Tree) - 中文](https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)\n",
    "- [how decision tree works - 英文](http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/)\n",
    "\n",
    "1. 在分類問題中，若沒有任何限制，決策樹有辦法在訓練時將 training loss 完全降成 0 嗎？\n",
    "2. 決策樹只能用在分類問題嗎？還是可以用來解決回歸問題？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.在沒有限制的情形下，決策樹能夠將training loss完全降為零，也就是將訓練樣本一路分切至每個leaf都只有同樣的一種類別，就可以確保training loss為0，但同時也將讓模型非常嚴重的overfit。\n",
    "https://kknews.cc/zh-tw/education/48blzm2.html\n",
    "https://www.jamleecute.com/decision-tree-cart-%E6%B1%BA%E7%AD%96%E6%A8%B9/\n",
    " \n",
    "2.https://codertw.com/%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80/564323/\n",
    "有些決策樹演算法也可以像迴歸分析一樣，預測的結果呈現的是一個實數(例如：身高、體重)，這種決策樹就稱為迴歸樹(Regression Tree)。 迴歸樹與分類樹的思路類似，但葉節點的資料型別不是離散型，而是連續型，對CART稍作修改就可以處理迴歸問題。CART演算法用於迴歸時根據葉子是具體指還是另外的機器學習模型又可以分為迴歸樹和模型樹。但無論是迴歸樹還是模型樹，其適用場景都是：標籤值是連續分佈的，但又是可以劃分群落的，群落之間是有比較鮮明的區別的，即每個群落內部是相似的連續分佈，群落之間分佈確是不同的。所以迴歸樹和模型樹既算迴歸，也稱得上分類。\n",
    "決策樹也可以用來處理回歸問題，在node展開的同時就可視為是將樣本空間劃分並給予預測的輸出數值，在選擇劃分方式時相較分類問題使用gini或entropy，回歸問題應該是選擇指定的損失函數最小的劃分方式，來將樣本空間做劃分。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
